{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1004e174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1078aaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Entropy Function\n",
    "def entropy(labels, base=None):\n",
    "    vc = pd.Series(labels).value_counts(normalize=True, sort=False)\n",
    "    base = 2 if base is None else base  # Fixed!\n",
    "    return -(vc * np.log(vc)/np.log(base)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74738721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Misclassification Function\n",
    "def misclassification(labels):\n",
    "    # Get the most common class\n",
    "    if isinstance(labels, pd.Series):\n",
    "        # Count how often each class appears\n",
    "        value_counts = labels.value_counts()\n",
    "        # Get the probability of the most common class\n",
    "        max_prob = value_counts.max() / len(labels)\n",
    "        # Misclassification impurity\n",
    "        return 1 - max_prob\n",
    "    else:\n",
    "        raise ValueError('Input must be a Pandas Series.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "098ef26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Gini Function\n",
    "def gini(y):\n",
    "    if isinstance(y, pd.Series):\n",
    "        p = y.value_counts() / y.shape[0]\n",
    "        gini = 1 - np.sum(p ** 2)\n",
    "        return gini\n",
    "    else:\n",
    "        raise ValueError('Input must be a Pandas Series.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d9adb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Gain Function\n",
    "def calculate_gain(subset_splits, criterion=\"gini\"):\n",
    "    # Step 1: Calculate total number of samples\n",
    "    total_samples = sum(len(labels) for _, labels in subset_splits)\n",
    "    \n",
    "    # Step 2: Combine all labels to calculate impurity before split\n",
    "    all_labels = pd.concat([labels for _, labels in subset_splits])\n",
    "    \n",
    "    # Step 3: Calculate impurity before split\n",
    "    if criterion == \"gini\":\n",
    "        impurity_before = gini(all_labels)\n",
    "    elif criterion == \"entropy\":\n",
    "        impurity_before = entropy(all_labels)\n",
    "    elif criterion == \"misclassification\":\n",
    "        impurity_before = misclassification(all_labels)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown criterion\")\n",
    "    \n",
    "    # Step 4: Calculate weighted average impurity after split\n",
    "    impurity_after = 0\n",
    "    for value, labels in subset_splits:\n",
    "        # Weight is the proportion of samples in this subset\n",
    "        weight = len(labels) / total_samples\n",
    "        \n",
    "        # Calculate impurity for this subset\n",
    "        if criterion == \"gini\":\n",
    "            subset_impurity = gini(labels)\n",
    "        elif criterion == \"entropy\":\n",
    "            subset_impurity = entropy(labels)\n",
    "        elif criterion == \"misclassification\":\n",
    "            subset_impurity = misclassification(labels)\n",
    "        \n",
    "        # Add weighted impurity\n",
    "        impurity_after += weight * subset_impurity\n",
    "    \n",
    "    # Step 5: Information gain = reduction in impurity\n",
    "    gain = impurity_before - impurity_after\n",
    "    \n",
    "    return gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f98f970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Dataset Split Function\n",
    "def dataset_split_by_feature(dataset, feature, label):\n",
    "    splits = []\n",
    "    \n",
    "    # Get all unique values of this feature\n",
    "    unique_values = dataset[feature].unique()\n",
    "    \n",
    "    # For each unique value, create a subset\n",
    "    for value in unique_values:\n",
    "        # Get rows where feature equals this value\n",
    "        subset = dataset[dataset[feature] == value]\n",
    "        # Get just the labels for this subset\n",
    "        subset_labels = subset[label]\n",
    "        # Store the value and its corresponding labels\n",
    "        splits.append((value, subset_labels))\n",
    "    \n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d867ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Find Best Split Function\n",
    "def find_best_split(dataset, label, criterion=\"gini\"):\n",
    "    # for each split calculate gain, return best feature to split and the gain\n",
    "    best_feature = None\n",
    "    best_gain = -1  # Start with -1 so any positive gain is better\n",
    "    \n",
    "    # Get all feature names (all columns except the label)\n",
    "    features = [col for col in dataset.columns if col != label]\n",
    "    \n",
    "    # Try splitting on each feature\n",
    "    for feature in features:\n",
    "        # Split the dataset by this feature\n",
    "        splits = dataset_split_by_feature(dataset, feature, label)\n",
    "        \n",
    "        # Calculate gain for this split\n",
    "        gain = calculate_gain(splits, criterion)\n",
    "        \n",
    "        # If this is the best gain so far, remember it\n",
    "        if gain > best_gain:\n",
    "            best_gain = gain\n",
    "            best_feature = feature\n",
    "    \n",
    "    return best_feature, best_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0edf4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    name = \"DeLaRiva_Skyye\"\n",
    "    os.makedirs(name=name, exist_ok=True)\n",
    "    \n",
    "    # TEST WITH BALLOONS FIRST\n",
    "    data = pd.read_csv(\"agaricus-lepiota.csv\")\n",
    "    \n",
    "    # Print to see what the data looks like\n",
    "    print(data.head())\n",
    "    print(f\"\\nColumns: {data.columns.tolist()}\")\n",
    "    \n",
    "    label = \"class\"\n",
    "    \n",
    "    criteria = [\"gini\", \"entropy\", \"misclassification\"]\n",
    "    f = open(f\"{name}/mushrooms.txt\", \"w\")\n",
    "    for criterion in criteria:\n",
    "        best_feature, best_gain = find_best_split(data, label, criterion)\n",
    "        print(f\"Best feature: {best_feature} Using: {criterion} - Gain: {best_gain:.2f}\")\n",
    "        f.write(f\"Best feature: {best_feature} Using: {criterion}-Gain: {best_gain:.2f}\\n\")\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
